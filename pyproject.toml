[project]
name = "localbricks"
dynamic = ["version"]
description = "Write code that works the same locally and in Databricks"
readme = "README.md"
authors = [
    { name = "Anders Poirel", email = "contact@anderspoirel.net" }
]
requires-python = ">=3.10"
dependencies = [
    "databricks-sdk>=0.36",
]

[project.optional-dependencies]
connect = ["databricks-connect>=13"]

[dependency-groups]
dev = [
    "mypy>=1.16.0",
    # we want regular Spark for unit tests, but in we don't want to install when
    # running on a Databricks cluster since it would interfere with the runtime's Spark.
    "pyspark[connect]==3.5.5",
    "pytest>=8.4.0",
    "ruff>=0.11.13",
    # pyspark.sql.connect used 'distutils' for a Pandas version check. In Python 3.12+
    # where distutils is removed from the standard library, use the version provided
    # by 'setuptools' instead. TODO: remove this once this is fixed in Spark release.
    'setuptools; python_version >= "3.12"',
]
docs = [
    "pydata-sphinx-theme>=0.15.0",
    "pyspark[connect]==3.5.5",
    'setuptools; python_version >= "3.12"',
    "sphinx>=7.0.0",
    "sphinx-autodoc-typehints>=1.25.0",
]

[build-system]
requires = ["flit_core>=3.2"]
build-backend = "flit_core.buildapi"

[tool.ruff]
line-length = 88

[tool.ruff.lint]
select = [
    "E",  # pycodestyle errors
    "W",  # pycodestyle warnings
    "F",  # pyflakes
    "D",  # pydocstyle
]
ignore = [
    # first line should be in imperative mood. IMO this check is often innacurate, or
    # results in awkward sentences.
    "D401",
    # incompatible with D211 (no-blank-line-before-class)
    "D203",
    # incompatible with D212 (multi-line-summary-first-line)
    "D213",
]

[tool.ruff.lint.per-file-ignores]
"test_*.py" = ["D"]
"docs/conf.py" = ["D"]
# don't need a module docstring for __init__.py
"__init__.py" = ["D104"]
